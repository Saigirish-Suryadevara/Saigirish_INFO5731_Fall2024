{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saigirish-Suryadevara/Saigirish_INFO5731_Fall2024/blob/main/Suryadevara_INFO5731_Exercise_3_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Sentiment analysis of customer reviews of a certain videogame console is a very interesting text classification problem.\n",
        "The objective is to categorize the reviews into three categories positive, negative, and neutral.\n",
        "This is a description of the five characterstics or features which are useful in building the machine learning model for this particular task:\n",
        "1. Bag of Words: It represents the frequency of words in the text without any order. It helps to identify if the words are associated with +ve or -ve segment\n",
        "2. Sentiment Lexicons: sentiment lexicons are pre defined words tagged with sentiment scores like +ve,-ve,neutral. Using these sentiment lexicons we can\n",
        "                       give sentiment scores to words.\n",
        "3. Tf-Idf: Tf-Idf tells how important a word is relative to the dataset. It disregards common used words and give high weigh to rarely used words.\n",
        "4. POS Tags: POS tags like noun,verb,adjective etc., help to understand the role of word in sentence. Nouns & pronoun give context, where as verb & adjective\n",
        "             bare sentiment.\n",
        "5. Readability Score: Readability score evaluates how complex the text is, this helps to determine the complexity of text which is helpful in sentiment analysis.\n",
        "By using these above listed features, we can build a machine learning model to differentiate and classify user reviews for game consoles into sentiment categories.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!! pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FBkO0WpeN4TZ",
        "outputId": "35d51cf0-d4d2-4b50-89e4-9daebe207422"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Collecting textstat',\n",
              " '  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)',\n",
              " 'Collecting pyphen (from textstat)',\n",
              " '  Downloading pyphen-0.16.0-py3-none-any.whl.metadata (3.2 kB)',\n",
              " 'Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (71.0.4)',\n",
              " 'Downloading textstat-0.7.4-py3-none-any.whl (105 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/105.1 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m105.1/105.1 kB\\x1b[0m \\x1b[31m5.5 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading pyphen-0.16.0-py3-none-any.whl (2.1 MB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/2.1 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m2.1/2.1 MB\\x1b[0m \\x1b[31m95.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m2.1/2.1 MB\\x1b[0m \\x1b[31m51.3 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hInstalling collected packages: pyphen, textstat',\n",
              " 'Successfully installed pyphen-0.16.0 textstat-0.7.4']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d90fcf-a785-4fca-f854-1177b31fda99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW features:\n",
            "    amazing  and  bad  build  console  consoles  controller  does  every  \\\n",
            "0        0    0    0      0        1         0           0     0      1   \n",
            "1        1    1    0      0        1         0           0     0      0   \n",
            "2        0    0    0      1        0         0           1     0      0   \n",
            "3        0    0    0      0        1         0           0     1      0   \n",
            "4        0    0    1      0        1         1           0     0      0   \n",
            "\n",
            "   feels  for  genration  good  in  is  mark  not  of  penny  perform  \\\n",
            "0      0    0          0     0   0   1     0    0   0      1        0   \n",
            "1      1    0          0     0   0   0     0    0   0      0        0   \n",
            "2      1    0          0     0   0   0     0    0   0      0        0   \n",
            "3      0    1          0     1   0   0     1    1   0      0        1   \n",
            "4      0    0          1     0   1   1     0    0   1      0        0   \n",
            "\n",
            "   premium  price  spectacular  the  this  upto  worse  worth  yet  \n",
            "0        0      0            1    0     1     0      0      1    0  \n",
            "1        1      0            0    1     0     0      0      0    0  \n",
            "2        0      0            0    1     0     0      1      0    0  \n",
            "3        0      1            0    3     0     1      0      0    1  \n",
            "4        0      0            0    0     2     0      1      0    0  \n",
            "\n",
            " Tf-Idf features:\n",
            "     amazing       and       bad     build   console  consoles  controller  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000  0.237665  0.000000    0.000000   \n",
            "1  0.475822  0.475822  0.000000  0.000000  0.268070  0.000000    0.000000   \n",
            "2  0.000000  0.000000  0.000000  0.516374  0.000000  0.000000    0.516374   \n",
            "3  0.000000  0.000000  0.000000  0.000000  0.154169  0.000000    0.000000   \n",
            "4  0.000000  0.000000  0.329281  0.000000  0.185511  0.329281    0.000000   \n",
            "\n",
            "       does     every     feels       for  genration      good        in  \\\n",
            "0  0.000000  0.421853  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.383890  0.000000   0.000000  0.000000  0.000000   \n",
            "2  0.000000  0.000000  0.416607  0.000000   0.000000  0.000000  0.000000   \n",
            "3  0.273649  0.000000  0.000000  0.273649   0.000000  0.273649  0.000000   \n",
            "4  0.000000  0.000000  0.000000  0.000000   0.329281  0.000000  0.329281   \n",
            "\n",
            "         is      mark       not        of     penny   perform   premium  \\\n",
            "0  0.340349  0.000000  0.000000  0.000000  0.421853  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.475822   \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "3  0.000000  0.273649  0.273649  0.000000  0.000000  0.273649  0.000000   \n",
            "4  0.265662  0.000000  0.000000  0.329281  0.000000  0.000000  0.000000   \n",
            "\n",
            "      price  spectacular       the      this      upto     worse     worth  \\\n",
            "0  0.000000     0.421853  0.000000  0.340349  0.000000  0.000000  0.421853   \n",
            "1  0.000000     0.000000  0.318664  0.000000  0.000000  0.000000  0.000000   \n",
            "2  0.000000     0.000000  0.345822  0.000000  0.000000  0.416607  0.000000   \n",
            "3  0.273649     0.000000  0.549798  0.000000  0.273649  0.000000  0.000000   \n",
            "4  0.000000     0.000000  0.000000  0.531323  0.000000  0.265662  0.000000   \n",
            "\n",
            "        yet  \n",
            "0  0.000000  \n",
            "1  0.000000  \n",
            "2  0.000000  \n",
            "3  0.273649  \n",
            "4  0.000000  \n",
            "\n",
            " Sentiment Lexicon features:\n",
            "                                              Review    pos    neg    neu\n",
            "0    This console is spectacular! Worth every penny.  0.268  0.000  0.732\n",
            "1             The console feels premium and amazing.  0.432  0.000  0.568\n",
            "2                  The controller build feels worse.  0.000  0.437  0.563\n",
            "3  The console does not perform upto the mark, ye...  0.195  0.000  0.805\n",
            "4  This console is worse! bad in this genration o...  0.000  0.463  0.537\n",
            "\n",
            " POS-tag :\n",
            "                                               review  \\\n",
            "0    This console is spectacular! Worth every penny.   \n",
            "1             The console feels premium and amazing.   \n",
            "2                  The controller build feels worse.   \n",
            "3  The console does not perform upto the mark, ye...   \n",
            "4  This console is worse! bad in this genration o...   \n",
            "\n",
            "                                             pos_tag  \n",
            "0  [(This, DT), (console, NN), (is, VBZ), (specta...  \n",
            "1  [(The, DT), (console, NN), (feels, NNS), (prem...  \n",
            "2  [(The, DT), (controller, NN), (build, NN), (fe...  \n",
            "3  [(The, DT), (console, NN), (does, VBZ), (not, ...  \n",
            "4  [(This, DT), (console, NN), (is, VBZ), (worse,...  \n",
            "\n",
            " Readability score for reviews :\n",
            "                                               review  read_ability_score\n",
            "0    This console is spectacular! Worth every penny.                 8.2\n",
            "1             The console feels premium and amazing.                 6.8\n",
            "2                  The controller build feels worse.                 0.5\n",
            "3  The console does not perform upto the mark, ye...                 3.6\n",
            "4  This console is worse! bad in this genration o...                 2.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from textstat import flesch_kincaid_grade\n",
        "\n",
        "\n",
        "# Downloading nltk resources\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Sample text dataset\n",
        "dataset ={'review': [\n",
        "    \"This console is spectacular! Worth every penny.\",\n",
        "    \"The console feels premium and amazing.\",\n",
        "    \"The controller build feels worse.\",\n",
        "    \"The console does not perform upto the mark, yet good for the price.\",\n",
        "    \"This console is worse! bad in this genration of consoles.\",\n",
        "]}\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "#1. Bag of words\n",
        "\n",
        "def BoW(column):\n",
        "  BoW_vectoriser = CountVectorizer()\n",
        "  BoW_matrix = BoW_vectoriser.fit_transform(column)\n",
        "  return pd.DataFrame(BoW_matrix.toarray(), columns=BoW_vectoriser.get_feature_names_out())\n",
        "\n",
        "#2. Tf- Idf\n",
        "def TfIdf(column):\n",
        "  TfIdf_vectoriser = TfidfVectorizer()\n",
        "  TfIdf_matrix = TfIdf_vectoriser.fit_transform(column)\n",
        "  return pd.DataFrame(TfIdf_matrix.toarray(), columns=TfIdf_vectoriser.get_feature_names_out() )\n",
        "\n",
        "#3. Sentiment Lexicon\n",
        "def sentiment_lexicon(text):\n",
        "  sentiment_analyse = SentimentIntensityAnalyzer()\n",
        "  sentiment_results = [sentiment_analyse.polarity_scores(Review) for Review in text ] # Calculate results for each review in dataset\n",
        "  return sentiment_results\n",
        "\n",
        "sentiment_score = sentiment_lexicon(df['review'])\n",
        "sentimentlexicon_df = pd.DataFrame(sentiment_score)\n",
        "sentimentlexicon_df['Review'] = df['review'] # adding review to dataframe\n",
        "sentimentlexicon_df = sentimentlexicon_df[['Review','pos','neg','neu']]\n",
        "\n",
        "#4. POS Tags\n",
        "def get_postags(df):\n",
        "  return df['review'].apply(lambda Review: pos_tag(word_tokenize(Review)))\n",
        "df['pos_tag']=get_postags(df)\n",
        "\n",
        "# Readability score\n",
        "def get_readability_score(df):\n",
        "  return df['review'].apply(lambda Review: flesch_kincaid_grade(Review))\n",
        "df['read_ability_score'] = get_readability_score(df)\n",
        "\n",
        "\n",
        "# Extracting and printing features\n",
        "BoW_df = BoW(df['review'])\n",
        "TfIdf_df = TfIdf(df['review'])\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(\"BoW features:\\n\",BoW_df)\n",
        "print(\"\\n Tf-Idf features:\\n\", TfIdf_df)\n",
        "print(\"\\n Sentiment Lexicon features:\")\n",
        "print(sentimentlexicon_df)\n",
        "print(\"\\n POS-tag :\\n\", df[['review','pos_tag']])\n",
        "print(\"\\n Readability score for reviews :\\n\", df[['review','read_ability_score']])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d160c4a3-ed07-4140-8b17-fdb5afba60f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the            0.242857\n",
            "this           0.174334\n",
            "console        0.169083\n",
            "feels          0.160100\n",
            "worse          0.136454\n",
            "is             0.121202\n",
            "build          0.103275\n",
            "controller     0.103275\n",
            "and            0.095164\n",
            "premium        0.095164\n",
            "amazing        0.095164\n",
            "every          0.084371\n",
            "penny          0.084371\n",
            "worth          0.084371\n",
            "spectacular    0.084371\n",
            "bad            0.065856\n",
            "consoles       0.065856\n",
            "genration      0.065856\n",
            "of             0.065856\n",
            "in             0.065856\n",
            "perform        0.054730\n",
            "good           0.054730\n",
            "price          0.054730\n",
            "not            0.054730\n",
            "does           0.054730\n",
            "mark           0.054730\n",
            "upto           0.054730\n",
            "for            0.054730\n",
            "yet            0.054730\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text dataset\n",
        "dataset ={'review': [\n",
        "    \"This console is spectacular! Worth every penny.\",\n",
        "    \"The console feels premium and amazing.\",\n",
        "    \"The controller build feels worse.\",\n",
        "    \"The console does not perform upto the mark, yet good for the price.\",\n",
        "    \"This console is worse! bad in this genration of consoles.\",\n",
        "]}\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# intiating Tfidfvector\n",
        "tfidf_vector = TfidfVectorizer()\n",
        "\n",
        "# Transforming the text data to get TF-IDF score\n",
        "tfidf_matrix = tfidf_vector.fit_transform(df['review'])\n",
        "\n",
        "# extracting feature names\n",
        "feature = tfidf_vector.get_feature_names_out()\n",
        "\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature)\n",
        "\n",
        "# Calculating average Tf-Idf score\n",
        "feature_significance = tfidf_df.mean(axis=0)\n",
        "\n",
        "#ranking features in descending order\n",
        "feature_rank = feature_significance.sort_values(ascending = False)\n",
        "\n",
        "# Printing feature ranks based on Tf-Idf score in descending order\n",
        "print(feature_rank)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!! pip install transformers torch scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OODxxeHtRt6l",
        "outputId": "b1ee5cfd-ed21-4d84-bc41-6a6bd988b126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)',\n",
              " 'Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)',\n",
              " 'Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)',\n",
              " 'Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)',\n",
              " 'Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)',\n",
              " 'Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)',\n",
              " 'Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)',\n",
              " 'Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)',\n",
              " 'Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)',\n",
              " 'Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)',\n",
              " 'Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)',\n",
              " 'Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)',\n",
              " 'Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)',\n",
              " 'Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)',\n",
              " 'Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)',\n",
              " 'Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)',\n",
              " 'Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)',\n",
              " 'Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)',\n",
              " 'Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)',\n",
              " 'Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)',\n",
              " 'Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)',\n",
              " 'Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)',\n",
              " 'Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)',\n",
              " 'Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)',\n",
              " 'Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)',\n",
              " 'Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)',\n",
              " 'Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00409e94-b7d5-4088-b3fa-fb5888306c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review  Similarity_Score\n",
            "2                  The controller build feels worse.          0.892113\n",
            "0    This console is spectacular! Worth every penny.          0.845559\n",
            "1             The console feels premium and amazing.          0.845115\n",
            "4  This console is worse! bad in this genration o...          0.837599\n",
            "3  The console does not perform upto the mark, ye...          0.825553\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Sample text dataset\n",
        "dataset ={'review': [\n",
        "    \"This console is spectacular! Worth every penny.\",\n",
        "    \"The console feels premium and amazing.\",\n",
        "    \"The controller build feels worse.\",\n",
        "    \"The console does not perform upto the mark, yet good for the price.\",\n",
        "    \"This console is worse! bad in this genration of consoles.\",\n",
        "]}\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# defining function to get bert embeddings\n",
        "def extract_berd_embedding(text):\n",
        "  token = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  model = BertModel.from_pretrained('bert-base-uncased')\n",
        "  inputs = token(text, return_tensors='pt', padding = True, truncation=True)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "  return outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "# Defining function to rank review based on query simliarty\n",
        "def simliarty_rank(query):\n",
        "  embeded_query = extract_berd_embedding([query])\n",
        "  embeded_review = extract_berd_embedding(df['review'].tolist())\n",
        "  similarity = cosine_similarity(embeded_query,embeded_review).flatten()\n",
        "\n",
        "  df['Similarity_Score'] = similarity\n",
        "  return df.sort_values(by='Similarity_Score', ascending= False)[['review','Similarity_Score']]\n",
        "\n",
        "# Test Query\n",
        "query = \"I hate this controller\"\n",
        "review_ranked = simliarty_rank(query)\n",
        "\n",
        "# Printing the ranked reviews\n",
        "print(review_ranked)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "The assignment on extracting text features was very helpful. I found it beneficial and using techniques like TF-IDF, Sentiment lexicon, BoW and Readability score\n",
        "were helpful in understanding how to transform text into features suitable for machine learning models.\n",
        "The main challenge i faced was in the second question, which took lot of time to integrate multiple features into one and esnure all features were extracted\n",
        "accurately. However, this eased to answer remaining questions in ranking features and imporatnce.\n",
        "In my prior experience as a data engineer, this assignment was relevant and helpful as text feature extraction and text tranformation are helpful downstream processing\n",
        "and ETL pipelines.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}